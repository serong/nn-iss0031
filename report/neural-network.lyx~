#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass report
\begin_preamble

                                                     
% Code color ------
\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newcommand*{\vUniversity}{TALLINN UNIVERSITY OF TECHNOLOGY}
\newcommand*{\vFaculty}{Faculty of Information Technology}
\newcommand*{\vDepartment}{Department of Computer Control}
\newcommand*{\vChair}{Chair of Automatic Control and Systems Analysis}

\newcommand*{\vAuthor}{Serkan Ongan}
\newcommand*{\vAuthorCode}{156395IASM}
\newcommand*{\vReportName}{Digit Recognition Using Neural Network Implemented in Python }
\newcommand*{\vClass}{ISS0031 Modelling and Identification}
\newcommand*{\vSupervisor}{Aleksei Tepljakov}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
theorems-chap
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing other 1.3
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_amsmath 2
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 30mm
\topmargin 25mm
\rightmargin 30mm
\bottommargin 25mm
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "backgroundcolor={\color{backcolour}},basicstyle={\footnotesize},breakatwhitespace=false,breaklines=true,captionpos=b,commentstyle={\color{codegreen}},keepspaces=true,keywordstyle={\color{magenta}},numbers=left,numbersep=5pt,numberstyle={\tiny\color{codegray}},showspaces=false,showstringspaces=false,showtabs=false,stringstyle={\color{codepurple}},tabsize=2"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{titlepage}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing single
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vUniversity 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
vFaculty
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 1.5in*
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vAuthor 
\backslash
 
\end_layout

\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vAuthorCode
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 0.2in*
\end_inset


\end_layout

\begin_layout Standard
\align center

\series bold
\size larger
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
scshape{
\backslash
vReportName}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing single
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vClass
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 1.2in
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing single
\noindent
\align right
Supervisor: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vSupervisor
\end_layout

\end_inset


\size large
 
\end_layout

\begin_layout Standard
\begin_inset VSpace 3.1in
\end_inset


\end_layout

\begin_layout Standard
\align center

\size large
Tallinn 2016 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{titlepage}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{roman} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{arabic} 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Aim of the project
\end_layout

\begin_layout Standard
The aim of the project is to get a better understanding of neural networks
 and to get familiar with Python programming language and its libraries
 as tools for neural networks, as well as their implementations.
 Therefore it includes both theoretical and practical information in those
 areas.
\end_layout

\begin_layout Standard
For the theoretical part, in addition to information about neural network,
 I'll also look into Python libraries and their features.
 And for the practical part, more information can be found in the following
 section.
\end_layout

\begin_layout Section
The problem and the approach
\end_layout

\begin_layout Standard
The practical problem to be solved for the project is 
\emph on
digit recognition using neural network implemented in Python programming
 language
\emph default
.
 
\end_layout

\begin_layout Standard
For training and test, I have used an already modified and prepared dataset
 (MNIST) that comes with statistics about certain machine learning tasks
 and their performance on the data, which I hope will offer a good benchmark
 for comparing performance.
\end_layout

\begin_layout Standard
For example, single layer neural network is shown to have an %8.4 error rate
 on test data, and a 2-layer network with total 300 hidden nodes using mean
 square error achieves %4.7 error.
 For my project I aim to reach a %5-10 error.
\end_layout

\begin_layout Standard
For Python libraries (which are discussed in Chapter 3), I plan to use a
 library that will offer a good balance between ease of use and 
\begin_inset Quotes eld
\end_inset

low level
\begin_inset Quotes erd
\end_inset

 implementation.
 I have noticed that an opinionated library like TensorFlow requires also
 a considerable amount of time to get familiar with the library itself,
 which would make it harder to stick to my goals for the project.
\end_layout

\begin_layout Chapter
Neural Networks
\end_layout

\begin_layout Standard
In this chapter, I take a brief look at neural networks in order to lay
 the foundations of the actual Python implementation of the project.
\end_layout

\begin_layout Section
Biological model
\end_layout

\begin_layout Standard
Taking a look at the biological model that inspired artificial neural networks
 can help us understand their structure better.
 But since the biological model of how neuron and brain work is highly complex,
 I think it's beneficial to make it a very brief look.
\end_layout

\begin_layout Standard
A typical neuron can be thought as a signal processing device.
 
\begin_inset CommandInset citation
LatexCommand cite
key "book:gurney"

\end_inset

 This happens through positively or negatively charged ions.
 Each neuron is connected to many others via synapses.
 Inputs to the neuron is summed up and if it's over a certain threshold
 the neuron is excited.
 In general, an neuron can be excited or inhibited.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Neuron structure.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/neuron_structure.jpg
	scale 75

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This basic model is very close to how a TLU (threshold logic unit) works.
 A neural network is formed of simplified version of biological neurons
 and their connections, and are capable of making approximations for given
 data, after necessary training.
\end_layout

\begin_layout Section
Basic theory
\end_layout

\begin_layout Standard
An artificial neural network is an assembly of interconnected simple processing
 elements (nodes), whose functionality is stored in the interconnections'
 strengths (weights).
 
\begin_inset CommandInset citation
LatexCommand cite
key "book:gurney"

\end_inset

 
\end_layout

\begin_layout Standard
Smallest element of a neural network is a node.
 Taking a look at TLU, a basic node structure, could make understanding
 how neural networks work easier.
\end_layout

\begin_layout Subsection
Threshold logic unit
\end_layout

\begin_layout Standard
A TLU unit can be explained as below.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
A TLU
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/tlu.png
	scale 35

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the above example, the sum of weighted inputs is called 
\series bold
activation.
 
\series default
An 
\series bold
activation function
\series default
 (a step function in this case) takes this activation and the node's bias
 (
\begin_inset Formula $\theta$
\end_inset

) and produces an output.
 So the output of a node depends on the weights, inputs, bias and the activation
 function type.
 
\end_layout

\begin_layout Subsection
Activation functions 
\end_layout

\begin_layout Standard
Activation functions affect the way node produces outputs, meaning they
 also affect the way information flows through the layers of the neural
 network.
 So we need to choose activation functions according to our goal and network
 structure.
 
\end_layout

\begin_layout Standard
For example, a sigmoid function would be a better choice if derivations
 are used.
 Two most used activation functions are hyperbolic tangent (eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hyperbolic"

\end_inset

) and sigmoid functions (eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sigmoid"

\end_inset

).
 
\begin_inset CommandInset citation
LatexCommand cite
key "book:heaton"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x)=\frac{e^{2x}-1}{e^{2x}+1}\label{eq:hyperbolic}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x)=\frac{1}{1+e^{-x}}\label{eq:sigmoid}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Sigmoid (left) and hyperbolic tangent functions.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/sigmoid.jpeg
	scale 40

\end_inset


\begin_inset space \quad{}
\end_inset


\begin_inset Graphics
	filename images/tanh.jpeg
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Another important activation function is called 
\series bold
Softmax
\series default
 which is used in the output layer to give output as a probability.
 More precisely it produces a K sized vector of real values in the range
 of (0, 1) that add up to 1 when summed.
\end_layout

\begin_layout Standard
This is helpful in tasks like digit recognition where output layer size
 is bigger than 1.
\end_layout

\begin_layout Subsection
Layers
\end_layout

\begin_layout Standard
After nodes, the next structure in a network is layers.
 A neural network has two special layers.
 Input and output layers, which depend on the data structure.
 The actual inner structure of the neural network depends on hidden layers
 between input and output layers, which effect the number of weights and
 therefore directly the training time of the network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
A neural network with 2 hidden layers.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/neural_net.jpeg
	scale 25

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Training
\end_layout

\begin_layout Standard
A neural network gives us a model derived from collected input and output
 data.
 But to achieve this, we need to 
\series bold
train
\series default
 the neural network.
 For neural networks, training means updating weights and biases according
 to the error in the each iteration of the training.
\end_layout

\begin_layout Standard

\series bold
Training dataset
\series default
 includes an input and an output for each example.
 During training, the training input is fed to the neural network and its
 output is compared to the training data output.
 With this comparison we acquire an error, and the
\emph on
 objective of training is minimizing this error
\emph default
.
\end_layout

\begin_layout Standard
So in its basics, training is the optimization of global error for the neural
 network.
\end_layout

\begin_layout Subsection
Cost function
\end_layout

\begin_layout Standard
Cost function can be thought as the objective function in optimization problems.
 It is a representation of the global error in the network and a measurement
 of how well a neural network fits to the entire training dataset.
 It also depends on weights and biases.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
C=F(w,\theta),\quad C\rightarrow min
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
There are several differents methods for cost function.
 But some basic cost functions are: 
\emph on
sum of square errors (eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sse"

\end_inset

), mean square error (eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mse"

\end_inset

), root mean square (eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:rms"

\end_inset

).
\end_layout

\begin_layout Standard
Although it should be noted that MSE is one of the most common cost functions
 used in neural networks.
 But there are exceptions, like the Levenberg Marquardt Algorithm requiring
 SSE.
 
\begin_inset CommandInset citation
LatexCommand cite
key "book:heaton"

\end_inset

 
\end_layout

\begin_layout Standard
A 
\series bold
linear error
\series default
 is simply the difference of ideal output (i) and actual output (a).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E=(i-a)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
MSE=\frac{1}{n}\sum_{i=1}^{n}E^{2}\label{eq:mse}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
SSE=\frac{1}{2}\sum_{p}E^{2}\label{eq:sse}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
RMS=\sqrt{\frac{1}{n}\sum_{i=1}^{n}E^{2}}\label{eq:rms}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Optimization
\end_layout

\begin_layout Standard
The aim of the training is to minimize the cost function.
 For this, optimization methods are used.
 One way to optimize the cost function is to use gradient descent.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Gradient descent on error function.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/descent.png
	scale 50

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Gradient descent is a means of taking a step in the direction that does
 the most to decrease our cost function.
 We use derivatives to decide which way to take a small step in order to
 make the cost function smaller.
 
\end_layout

\begin_layout Standard
For example, for C that 
\begin_inset Formula $C(v_{1},v_{2})$
\end_inset

, making small changes in 
\begin_inset Formula $v_{1}$
\end_inset

 and 
\begin_inset Formula $v_{2}$
\end_inset

 would cause a small changes in C.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\triangle C\equiv\frac{\delta C}{\delta v_{1}}\triangle v_{1}+\frac{\delta C}{\delta v_{2}}v_{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\triangle v=\left[\begin{array}{c}
\triangle v_{1}\\
\triangle v_{2}
\end{array}\right]\; and\;\nabla C=\left[\begin{array}{c}
\frac{\delta C}{\delta v_{1}}\\
\frac{\delta C}{\delta v_{2}}
\end{array}\right]
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The amount of step we need to take to make C smaller can be calculated as
 the following formula, where 
\begin_inset Formula $\eta$
\end_inset

 is the learning rate and 
\begin_inset Formula $\eta>0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\triangle v=-\eta\nabla C
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
But our initial starting point can make the optimization stop in a local
 minimum, and for that reason it's sometimes a good idea to train the network
 with different starting points.
\end_layout

\begin_layout Subsubsection
- Some optimization methods: 
\end_layout

\begin_layout Enumerate

\emph on
Backpropogation method:
\emph default
 A very common gradient descent based method.
 It requires a desired output for each input value, therefore it's more
 of a supervised learning tool.
 Gradient descent based methods often find the best solution within their
 starting point, so achieving a global result (finding the global minimum)
 is often has randomness to it.
\end_layout

\begin_layout Enumerate

\emph on
Genetic algorithm:
\emph default
 It is a global optimization method.
 Since it searches in many directions, the probability of finding a global
 minimum increases.
 And it makes it a very efficient tool for difficult non-linear functions.
\end_layout

\begin_layout Enumerate

\emph on
Simulated annealing:
\emph default
 SA is based on physical annealing process where physical substances go
 from a higher state of energy to a smaller one.
 A concrete example is heating metal to molten state and letting it slowly
 cooldown until it becomes solid again.
 An in application to neural networks, one distinct difference is that SA
 depends on user defined parameters in contrast to GA's dynamically assigned
 ones.
 Also it permits increasing 
\begin_inset Quotes eld
\end_inset

energy state
\begin_inset Quotes erd
\end_inset

, meaning accepting points on cost function with higher error values.
 This helps escaping local minimimums.
 
\begin_inset CommandInset citation
LatexCommand cite
key "bib:annealing"

\end_inset


\end_layout

\begin_layout Chapter
Python
\end_layout

\begin_layout Standard
This chapter includes information about certain Python libraries that offer
 tools to implement neural networks.
 It concludes with a section about the comparison of using computer vision
 solution instead of neural networks.
\end_layout

\begin_layout Section
Language and libraries
\end_layout

\begin_layout Standard
Python is a high level scripting language that supports several programming
 paradigms, including procedural and object-oriented, as well as a partial
 support for functional programming.
 This makes it a very flexible tool to use in different application areas.
\end_layout

\begin_layout Standard
Another advantage of Python is that it has a clear and almost pseudo-code
 like syntax, making it easier to read and code.
 Below can be seen some examples of the syntax.
 
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

\begin_inset Caption

\begin_layout Plain Layout
Python programming language.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

def function_name(args):
\end_layout

\begin_layout Plain Layout

	""" Function definition."""
\end_layout

\begin_layout Plain Layout

	return value
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# List comprehensions: a fast and convenient way to create lists.
\end_layout

\begin_layout Plain Layout

[x for x in range(1,10)]     #[1, 2, 3, 4, 5, 6, 7, 8, 9] 
\end_layout

\begin_layout Plain Layout

[x*x for x in range(1,10)]   # [1, 4, 9, 16, 25, 36, 49, 64, 81] 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Being popular and open-sourced, the language has plenty of libraries (roughly
 90000 libraries 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:pypi"

\end_inset

 ) and it is a widely used tool in scientific programming area.
\end_layout

\begin_layout Standard
For implementation, I have looked into following libraries.
\end_layout

\begin_layout Subsection*
NumPy
\end_layout

\begin_layout Standard
NumPy is one of the fundamental scientific packages in Python.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:numpy"

\end_inset

 It offers powerful N-dimensional objects and tools for integrating with
 C/C++ and Fortran code, which make it possible to solve performance issues
 using lower level languages.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

\begin_inset Caption

\begin_layout Plain Layout
Very brief look into NumPy
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

vector_4 = np.random.randn(4)
\end_layout

\begin_layout Plain Layout

# array([ 0.00695554, -0.28457662,  2.36434078, -0.05432569]) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# reshaping into a 2x2 matrix.
\end_layout

\begin_layout Plain Layout

vector_2x2 = vector_4.reshape(2,2)
\end_layout

\begin_layout Plain Layout

# array([[ 0.00695554, -0.28457662],        
\end_layout

\begin_layout Plain Layout

#        [ 2.36434078, -0.05432569]]) 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
When compared to 
\series bold
Matlab
\series default
, there are fundamental differences.
\end_layout

\begin_layout Enumerate
Basic type in NumPy are multidimensional arrays, and operations on these
 arrays are 
\bar under
element-wise
\bar default
.
 If matrix-like operations are needed, a sub-type called 
\series bold
matrix
\series default
 type should be used.
\end_layout

\begin_layout Enumerate
Python arrays are 0-indexed, so the first element of an array A can be reached
 by A[0].
\end_layout

\begin_layout Enumerate
Better support for GUI applications via Python's general purpose programming
 approach.
\end_layout

\begin_layout Enumerate
In contrast to Matlab, arrays have pass-by-reference semantic.
\end_layout

\begin_layout Standard
It should also be noted that official advice of the library is to use 
\series bold
arrays
\series default
 instead of the 
\series bold
matrix
\series default
 sub-type.
 While matrices behave more-or-less like Matlab matrices, they have disadvantage
s like having maximum 2 dimensions.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:numpyDOC"

\end_inset


\end_layout

\begin_layout Subsection*
Matplotlib
\end_layout

\begin_layout Standard
Matplotlib is an open-source 2-D plotting library that is widely used by
 Python applications.
\end_layout

\begin_layout Subsection*
TensorFlow
\end_layout

\begin_layout Standard
TensorFlow is an open-source library developed within Google's Machine Intellige
nce research organization, as a tool to make research in machine learning
 quicker and easier to transition from prototype to production.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:tensor"

\end_inset

 
\end_layout

\begin_layout Standard
TensorFlow is a system where you represent computations as graphs.
 Each operation (node) on the flow takes 
\series bold
tensors
\series default
.
 (In TensorFlow, a tensor is mainly a multi-dimensional array with static
 type and dynamic dimensions.) And each operation produces tensors.
\end_layout

\begin_layout Subsubsection*
- Some features: 
\end_layout

\begin_layout Enumerate
Backed by Google.
\end_layout

\begin_layout Enumerate
Good documentation, but complex.
\end_layout

\begin_layout Enumerate
Multi GPU support.
\end_layout

\begin_layout Enumerate
Distributed training.
\end_layout

\begin_layout Enumerate
Model checkpoints allows for pausing the training, evaluating and continuing
 from the checkpoint.
\end_layout

\begin_layout Subsection*
PyBrain
\end_layout

\begin_layout Standard
PyBrain is short for 
\emph on
Python-Based Reinforcement Learning, Artificial Intelligence and Neural
 Network Library
\emph default
, and it's a modular tool that offers easy to use algoritms for machine
 learning operations.
 They aim to be a library that can be used both entry-level students and
 researchers, so providing a less dense alternative to libraries like TensorFlow.
\end_layout

\begin_layout Subsubsection*
- Features: 
\end_layout

\begin_layout Enumerate
Support for standard and advanced algorithms in many areas of machine intelligen
ce.
 (Supervised and unsupervised learning, reinforcement learning, optimizations,
 neural networks, plotting etc.)
\end_layout

\begin_layout Enumerate
Allows complex architectures.
\end_layout

\begin_layout Subsection*
Theano
\end_layout

\begin_layout Standard
Theano is developed in University of Montreal's Institute of Learning Algorithms.
 It's used in the university's research as well as its machine learning
 classes.
 In general, it's a library used to define, optimize and evaluate expression
 related to multi-dimensional arrays.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:theano"

\end_inset

 
\end_layout

\begin_layout Subsubsection*
- Features: 
\end_layout

\begin_layout Enumerate
NumPy integration
\end_layout

\begin_layout Enumerate
Faster operations with GPU usage.
\end_layout

\begin_layout Enumerate
Efficient derivations
\end_layout

\begin_layout Enumerate
Extensive testing suite.
\end_layout

\begin_layout Standard
If offers speeds closer to compiled C code implementations, and it comes
 with several optimizations for symbolic expressions.
 ( 
\begin_inset Formula $x*y/y->x$
\end_inset

 )
\end_layout

\begin_layout Subsection*
ScikitLearn
\end_layout

\begin_layout Standard
Scikit-Learn is a machine learning library built upon NumPy, SciPy and matplotli
b libraries.
 It can handle both supervised and unsupervised learning, and supports following
 tasks: 
\emph on
Classification, regression, clustering, dimensionality reduction, model
 selection, data preprocessing.
\end_layout

\begin_layout Standard
Its advantage lies in the fact that it has extensive documentation and a
 lot of contribution from experts in machine learning field.
 It also provides a consisten interface to most machine learning tasks in
 a single library.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:scikitWhy"

\end_inset


\end_layout

\begin_layout Subsection*
Keras
\end_layout

\begin_layout Standard
Keras is a library dedicated to neural network tasks.
 It can run on TensorFlow or Theano libraries, therefore providing their
 advantages like speed, GPU usage and so on.
 Its focus is on being a library that enables fast experimentations.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:keras"

\end_inset


\end_layout

\begin_layout Subsubsection*
- Features: 
\end_layout

\begin_layout Enumerate
Supports both convolutional and recurrent neural networks, as well as combinatio
n of both.
\end_layout

\begin_layout Enumerate
GPU support.
\end_layout

\begin_layout Enumerate
Fast prototyping.
\end_layout

\begin_layout Enumerate
Minimalist.
\end_layout

\begin_layout Subsection*
Libraries with Python bindings
\end_layout

\begin_layout Standard
In addition to the above libraries, there are libraries that were written
 in other languages but offers Python bindings:
\end_layout

\begin_layout Enumerate
FANN: Fast Artificial Neural Network Library
\end_layout

\begin_layout Enumerate
Caffe
\end_layout

\begin_layout Enumerate
mxnet
\end_layout

\begin_layout Section
Computer vision alternative (OpenCV)
\end_layout

\begin_layout Standard
Another approach to digit recognition can be done using a computer vision
 library like OpenCV.
 OpenCV is a cross-platform computer vision library that offers interface
 for Java, C++, Python and C.
 It's possible to use classifiers like k-nearest neighbor, support vector
 machine and random forest.
\end_layout

\begin_layout Standard
MNIST database offers a list of machine learning tools and their performances
 on the MNIST datasets.
 This can give us an idea about the comparison between neural network and
 some methods used in computer vision.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Method comparison.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace medskip*
\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Methods
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Test Error Range
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Neural Networks
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
% 0.35 - 4.7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Convolutional Networks
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
% 0.23 - 1.7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
K-Nearest Neighbor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
% 0.52 - 5.0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Support Vector Machines
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
% 0.56 - 1.4
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Implementation
\end_layout

\begin_layout Section
Data
\end_layout

\begin_layout Standard
The data used in the project was acquired at the Modified NIST database
 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:mnist"

\end_inset

, which is a subset of a larger study done at the National Institute of
 Standards and Technology of United States of America.
\end_layout

\begin_layout Standard
It is preprocessed and formatted, therefore offering a good learning tool.
\end_layout

\begin_layout Subsection*
Data preparation and format
\end_layout

\begin_layout Standard
Each digit image was centered in 28x28 pixel format and later transformed
 into a vector.
 Each vector element represents a pixel in the image in the form of 
\series bold
blackness
\series default
 of the said pixel.
 Also each image was labeled in the form of vector with a size of 10.
\end_layout

\begin_layout Subsection*
Input
\end_layout

\begin_layout Standard
Each input to the neural network will be in the form a vector (784x1).
 A sample input if represented as a 28x28 matrix could look as follows:
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Number matrix representation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Plain Layout
\paragraph_spacing double
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
R/C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
...
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
28
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
.
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
28
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
...
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
And if plotted according to each pixel's blackness value, such a number
 could look as follows:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Number visual representation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/number_rep.png
	scale 50

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Output
\end_layout

\begin_layout Standard
Each number in the set is also labelled by a vector of (10x1) size.
 So, for example label for the above number would be:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

\begin_inset Caption

\begin_layout Plain Layout
Vector for number 4 in Python script
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

# Vector for number 4
\end_layout

\begin_layout Plain Layout

number_4 = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Datasets
\end_layout

\begin_layout Standard
The dataset comes with a training set of 70000 examples and a test set of
 10000 examples.
 For the project training set was partioned into 50000 examples for training
 and 20000 examples for testing and validation.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Datasets
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Dataset
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Inputs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Labels
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
training dataset
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
validation dataset
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
testing dataset
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10000
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
The neural network 
\end_layout

\begin_layout Standard

\series bold
Layers 
\series default
have been choosing in the following way: 784, 30, 10.
 I found that a single 30-node hidden layer was a balanced number, especially
 when considering the large input size.
\end_layout

\begin_layout Standard
Several 
\series bold
cost functions
\series default
 were tried for different implementations, but to be consistent I tried
 to use MSE whenever I could.
\end_layout

\begin_layout Standard
For the 
\series bold
optimization method
\series default
 variations of gradient descent method were used.
\end_layout

\begin_layout Section
Numpy based implementation
\end_layout

\begin_layout Standard
For the codes used in this implementation, 
\emph on
Neural Networks and Deep Learning
\emph default
 reference was my main reference 
\begin_inset CommandInset citation
LatexCommand cite
key "ref:nn"

\end_inset

, especially for 
\series bold
backpropogation
\series default
 and 
\series bold
stochastic gradient descent
\series default
 algorithms.
\end_layout

\begin_layout Subsection
Usage
\end_layout

\begin_layout Standard
The FeedForward class implements a neural network with stochastic gradient
 descent optimization and backpropogation, with somewhat satisfying results.
 Its only dependency is NumPy, which is used for matrix multiplications
 and similar actions.
\end_layout

\begin_layout Standard
The class can be called and used as following:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

\begin_inset Caption

\begin_layout Plain Layout
Numpy based implementation.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

# Creating a feed forward neural network with 
\end_layout

\begin_layout Plain Layout

# Input	: 784 nodes
\end_layout

\begin_layout Plain Layout

# Hidden   : 30 nodes
\end_layout

\begin_layout Plain Layout

# Output   : 10 nodes
\end_layout

\begin_layout Plain Layout

net = FeedForward([784, 30, 10]) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Training for 10 epochs with 3.0 learning rate.
 
\end_layout

\begin_layout Plain Layout

net.train(training_data, 10, 3.0, test_data)
\end_layout

\begin_layout Plain Layout

net.save_state()
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Result
\end_layout

\begin_layout Standard
This network can get good match results with relatively small amount of
 epochs.
 After 10 epochs, it reached a %94.12 match rate.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Epoch vs matches
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/base_epoch.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
PyBrain implementation
\end_layout

\begin_layout Subsection
Usage
\end_layout

\begin_layout Standard

\emph on
FeedForwardPB
\emph default
 class can be used to implement this neural network in the following way:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

\begin_inset Caption

\begin_layout Plain Layout
PyBrain implementation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

from feed_forward_pybrain import FeedForwardPB
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Creating the neural network with default values.
 
\end_layout

\begin_layout Plain Layout

# [784, 30, 10] as layers.
 
\end_layout

\begin_layout Plain Layout

nn = FeedForwardPB() 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Updating learning rate for training.
\end_layout

\begin_layout Plain Layout

nn.update_learningrate(0.5)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Training for 5 times.
 
\end_layout

\begin_layout Plain Layout

nn.train(5)
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Result
\end_layout

\begin_layout Standard
Maximum match reached with this implementation was 
\series bold
%90.62
\series default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
PyBrain implementation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/pb_epoch.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Keras implementation
\end_layout

\begin_layout Standard
This implementation is based on TensorFlow library, so it offers easy to
 use wrappers around TensorFlow and also comes with really good speed improvemen
ts over the other two options.
\end_layout

\begin_layout Subsection
Usage
\end_layout

\begin_layout Standard

\emph on
FeedForwardKeras
\emph default
 class can be used to implement a neural network as follows:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python"
inline false
status open

\begin_layout Plain Layout

\begin_inset Caption

\begin_layout Plain Layout
Keras implementation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

from feed_forward_keras import FeedForwardKeras
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Create a default neural network.
\end_layout

\begin_layout Plain Layout

# Layers       : 784, 30, 10
\end_layout

\begin_layout Plain Layout

# Optimization : RMSProp
\end_layout

\begin_layout Plain Layout

# Cost Func.
   : MSE
\end_layout

\begin_layout Plain Layout

nn = FeedForwardKeras()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Train for 5 epochs.
\end_layout

\begin_layout Plain Layout

nn.train(5, verbose=1) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Evaluate matches.
\end_layout

\begin_layout Plain Layout

nn.evaluate()
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Result
\end_layout

\begin_layout Standard
With this implementation, %95.43 match could be reached after 10 epochs.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Keras implementation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/keras_epochs.png
	scale 75

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Conclusion
\end_layout

\begin_layout Standard
The first approach, using just 
\series bold
Numpy
\series default
, was a very good way to get a better understanding of how neural networks
 work.
 I was able to see how different algorithms could be implemented and to
 have easier access to neural network parameters and so on.
 
\end_layout

\begin_layout Standard
However, for experimenting and prototyping a library would be a better option.
 So for next option, I checked two libraries: 
\emph on
PyBrain
\emph default
 and 
\emph on
Keras
\emph default
.
\end_layout

\begin_layout Standard

\series bold
PyBrain
\series default
 implementation was the slowest and least performant of all three implementation
s.
 Epochs took around 2 minutes, making configuring network parameters require
 a little more patient.
 
\end_layout

\begin_layout Standard
PyBrain, I had initially assumed, was the most 
\begin_inset Quotes eld
\end_inset

low level
\begin_inset Quotes erd
\end_inset

 library among the options, therefore would be a good next step after Numpy
 implementation.
 But I found working with Keras much more pleasant.
\end_layout

\begin_layout Standard

\series bold
Keras
\series default
 can use TensorFlow as its backend, so it provides an easy to use interface
 to TensorFlow and more importantly take advantage of its speed.
 It was quite easy to try new optimization algorithms and cost functions.
\end_layout

\begin_layout Standard
Next time, I would mostly likely start directly with Keras and later move
 on to TensorFlow.
 
\end_layout

\begin_layout Standard
In summary, I have found Python and its libraries very pleasant to work
 with data and neural network.
 Libraries like Numpy, SciPy and Matplotlib are very mature and capable,
 and they provide stable foundations for other libraries to built-upon.
\end_layout

\begin_layout Standard
One thing I missed from Matlab was the training GUI of neural network module,
 where you can get easy access to graphs for performance and other parameters.
 Although it's theoretically possible to do the same thing with Python,
 it's often a more CLI-driven development, and it's up to the user to create
 the visuals and outputs they need.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:mnist"

\end_inset

LeCun, Y.
 
\emph on
Mnist database of handwritten digits
\emph default
, http://yann.lecun.com/exdb/mnist/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:pypi"

\end_inset


\emph on
PyPi, Python Library
\emph default
, https://pypi.python.org/pypi
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:numpy"

\end_inset


\emph on
Numpy
\emph default
, http://www.numpy.org/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:numpyDOC"

\end_inset


\emph on
Numpy
\emph default
 
\emph on
documentation
\emph default
, https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:matplotlib"

\end_inset


\emph on
Matplotlib
\emph default
, http://matplotlib.org/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:tensor"

\end_inset


\emph on
TensorFlow
\emph default
, https://www.tensorflow.org/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:tensorGood"

\end_inset

Kuster, D., 
\emph on
The good, bad and ugly of TensorFlow
\emph default
, 
\emph on
KD Nuggets
\emph default
, 2015
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:pybrain"

\end_inset


\emph on
PyBrain
\emph default
, http://pybrain.org/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:theano"

\end_inset


\emph on
Theano
\emph default
, http://deeplearning.net/software/theano/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:scikit"

\end_inset


\emph on
Scikit-learn
\emph default
, http://scikit-learn.org/stable/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:scikitWhy"

\end_inset

Lorica, B., 
\emph on
Six reasons why I recommend scikit-learn
\emph default
, https://www.oreilly.com/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:keras"

\end_inset


\emph on
Keras
\emph default
, https://keras.io/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:opencv"

\end_inset

OpenCV, http://opencv.org/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "bib:neiger"

\end_inset

Neiger, V.
 
\emph on
Handwritten digits recognition using openCV
\emph default
, Western University, (2015)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "book:gurney"

\end_inset

Gurney, K.
 An introduction to neural networks, UCL-Press (1997)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "book:heaton"

\end_inset

Heaton, J.
 Introduction to math of neural networks, Heaton Research, (2012)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "bib:annealing"

\end_inset

Rere, L.M., Fanany, M.I, Arymurthy, A.M., Simulated annealing algorithm for deep
 learning, Procedia Computer Science, Volume 72, (2015) Pages 137-144
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ref:nn"

\end_inset

Nielsen, M, Neural Networks and Deep Learning, 2016, http://neuralnetworksanddee
plearning.com/
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Bibliography}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
